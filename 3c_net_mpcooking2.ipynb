{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c-net_mpcooking2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1bE6OPamvsKua2i42_lPrB54qyQYF4uGP",
      "authorship_tag": "ABX9TyOifjGygFyNJqynvpf9/jKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDesai11/3c-net_on_mpcooking2/blob/main/3c_net_mpcooking2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xH6g4S17jjO"
      },
      "source": [
        "Implement 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization paper on mpcooking2 dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC_fFSvx6T8n"
      },
      "source": [
        "!git clone https://github.com/naraysa/3c-net \n",
        "!pip install tensorboard_logger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjSGI3sSXMr-"
      },
      "source": [
        "# Change model.py\n",
        "\n",
        "```\n",
        "#base_x_f = inputs[:,:,1024:]\n",
        "base_x_f = inputs[:,:,128:]\n",
        "#base_x_r = inputs[:,:,:1024]\n",
        "base_x_r = inputs[:,:,:128]\n",
        "```\n",
        "\n",
        "# Upload read_cooking_dataset.py Class file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoRbpcF4WsN",
        "outputId": "d239392e-1f82-4107-fbcb-e7a46b76859b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# TESTING CELL \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "'''\n",
        "\n",
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "a = np.load('/content/3c-net/Thumos14-Annotations/classlist.npy')\n",
        "b = np.load('/content/3c-net/Thumos14-Annotations/labels.npy')\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old\n",
        "\n",
        "print(a)\n",
        "\n",
        "\n",
        "#import pickle\n",
        "\n",
        "#reza_dataset = pickle.load(open('/content/drive/My Drive/MPCooking 2/mpcooking2_data_pca.pickle', \"rb\"))\n",
        "\n",
        "print reza_dataset.train[0][0] # video name\n",
        "#print reza_dataset.train[0][1].shape # action label per frame\n",
        "#print np.array([item[1] for item in reza_dataset.train]).shape\n",
        "#print len(reza_dataset.train)\n",
        "#print reza_dataset.train[0][2] # actions order label\n",
        "#print(reza_dataset.train[0][3].shape)# rgb features\n",
        "#print(reza_dataset.train[0][4].shape) # flow features\n",
        "print(len(reza_dataset.dish_classlist)) # video name\n",
        "print(utils.strlist2multihot([reza_dataset.dish_classlist[0]],reza_dataset.dish_classlist))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9f5bffa8f65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrlist2multihot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdish_classlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdish_classlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m '''\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreza_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reza_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0fsv2ETu2hu"
      },
      "source": [
        "# Cell to import dataset for checking errors in model\n",
        "\n",
        "import pickle\n",
        "\n",
        "print('loading pickle')\n",
        "reza_dataset = pickle.load(open('/content/drive/My Drive/MPCooking 2/mpcooking2_data_pca.pickle', \"rb\"))\n",
        "print('pickle loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCWXHH_R6Vom"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import utils\n",
        "\n",
        "class ConvertDataset:\n",
        "     def __init__(self, reza_dataset, args):\n",
        "         self.dataset_name = args.dataset_name\n",
        "         self.feature_size = 256\n",
        "         self.num_class = reza_dataset.nDishes # number of class = number of dishes in our case (59dishes)\n",
        "         self.classlist = reza_dataset.dish_classlist\n",
        "         self.t_max = None\n",
        "         self.is_training = False\n",
        "         self.batch_size = args.batch_size\n",
        "         self.train = reza_dataset.train\n",
        "         self.test = reza_dataset.test\n",
        "         self.currenttestidx = 0\n",
        "         self.testidx = [i for i in range(len(reza_dataset.test))]\n",
        "         #path_to_annotations\n",
        "         #lst_valid\n",
        "         self.labels101to20 = np.arange(0,59)\n",
        "         self.currenttrainidx = 0\n",
        "        \n",
        "     def load_data(self, is_training=True):\n",
        "\n",
        "        features = []\n",
        "        labels = []\n",
        "        count_labels = []\n",
        "        if is_training:\n",
        "\n",
        "            # Not performing random sampling here, going through training data in order\n",
        "            if self.currenttrainidx + self.batch_size < len(self.train):\n",
        "                batch_end = self.batch_size\n",
        "            else:\n",
        "                batch_end = len(self.train)\n",
        "\n",
        "            for i in range(self.currenttrainidx, self.currenttrainidx + batch_end):\n",
        "\n",
        "                # get label for recipe from video name ex: s02-d72 72 is dish index. get dish name from dish_index2label(dish index)\n",
        "                label = self.train[i][0].split('-')[1].replace('d','')\n",
        "                dish_name = reza_dataset.dish_index2label[label]\n",
        "                labels.append([dish_name])\n",
        "\n",
        "                # concat the rgb and flow features\n",
        "                flow = self.train[i][4] # flow features\n",
        "                rgb = self.train[i][3] # rgb features\n",
        "                features.append(np.concatenate((flow, rgb)).T) #transpose features to get (length x num_features)\n",
        "\n",
        "                # our recipe is the action so count for each is 1\n",
        "                count_labels.append(1)\n",
        "            \n",
        "            labels_multihot = [utils.strlist2multihot(labs,self.classlist) for labs in labels]\n",
        "            \n",
        "            return np.array([utils.process_feat(feature, self.t_max) for feature in features]), np.array(labels_multihot), np.array(count_labels)\n",
        "\n",
        "        else:\n",
        "            testing_data = self.test[self.currenttestidx]\n",
        "\n",
        "            # getting the label for that testing data\n",
        "            string_label = reza_dataset.dish_index2label[testing_data[0].split('-')[1].replace('d','')]\n",
        "            labs = utils.strlist2multihot([string_label],self.classlist)\n",
        "\n",
        "            # getting the features for that testing data\n",
        "            flow = testing_data[4] # flow features\n",
        "            rgb = testing_data[3] # rgb features\n",
        "            feat = np.concatenate((flow, rgb)).T\n",
        "\n",
        "            if self.currenttestidx == len(self.testidx)-1:\n",
        "                done = True; self.currenttestidx = 0\n",
        "            else:\n",
        "                done = False; self.currenttestidx += 1\n",
        "         \n",
        "            return np.array([feat]), np.array(labs), done\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afpbEs6M6kPu",
        "outputId": "bca40373-3706-4f6e-cf51-cf68d150f0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from model import Model\n",
        "from video_dataset import Dataset\n",
        "from test import test\n",
        "from train import train\n",
        "from tensorboard_logger import Logger\n",
        "import options\n",
        "from center_loss import CenterLoss\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "import torch.optim as optim\n",
        "\n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = options.parser.parse_args(['--dataset-name', 'MP_Cooking_2'])\n",
        "    \n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "    # Change T_MAX --\n",
        "    t_max = 750\n",
        "    t_max_ctc = 7500\n",
        "    if args.activity_net:\n",
        "        t_max = 200\n",
        "        t_max_ctc = 400\n",
        "\n",
        "\n",
        "    #dataset = Dataset(args)\n",
        "    dataset = ConvertDataset(reza_dataset, args)\n",
        "    \n",
        "    #Change save directory to drive to avoid loss of data\n",
        "    os.system('mkdir -p ./drive/My Drive/MPCooking 2/ckpt/')\n",
        "    os.system('mkdir -p ./drive/My Drive/MPCooking 2/logs/' + args.model_name)\n",
        "    logger = Logger('./drive/My Drive/MPCooking 2/logs/' + args.model_name)\n",
        "    \n",
        "    #model = Model(dataset.feature_size, dataset.num_class, dataset.labels101to20).to(device)\n",
        "    model = Model(dataset.feature_size, dataset.num_class).to(device)\n",
        "\n",
        "    if args.eval_only and args.pretrained_ckpt is None:\n",
        "        print('***************************')\n",
        "        print('Pretrained Model NOT Loaded')\n",
        "        print('Evaluating on Random Model')\n",
        "        print('***************************')\n",
        "\n",
        "    if args.pretrained_ckpt is not None:\n",
        "        model.load_state_dict(torch.load(args.pretrained_ckpt))\n",
        "\n",
        "    best_acc = 0\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0.0005)  \n",
        "    #criterion_cent_f = CenterLoss(num_classes=dataset.num_class, feat_dim=1024, use_gpu=True)\n",
        "    criterion_cent_f = CenterLoss(num_classes=dataset.num_class, feat_dim=128, use_gpu=True)\n",
        "    optimizer_centloss_f = torch.optim.SGD(criterion_cent_f.parameters(), lr=0.1)\n",
        "    #criterion_cent_r = CenterLoss(num_classes=dataset.num_class, feat_dim=1024, use_gpu=True)\n",
        "    criterion_cent_r = CenterLoss(num_classes=dataset.num_class, feat_dim=128, use_gpu=True)\n",
        "    optimizer_centloss_r = torch.optim.SGD(criterion_cent_r.parameters(), lr=0.1)\n",
        "\n",
        "    criterion_cent_all=[criterion_cent_f, criterion_cent_r]\n",
        "    optimizer_centloss_all=[optimizer_centloss_f, optimizer_centloss_r]\n",
        "\n",
        "    for itr in range(args.max_iter):\n",
        "        dataset.t_max = t_max\n",
        "        if itr % 2 == 0 and itr > 000:\n",
        "            dataset.t_max = t_max_ctc\n",
        "        if not args.eval_only:\n",
        "            train(itr, dataset, args, model, optimizer, criterion_cent_all, optimizer_centloss_all, logger, device)\n",
        "          \n",
        "        if itr % 500 == 0 and (not itr == 0 or args.eval_only):\n",
        "            acc = test(itr, dataset, args, model, logger, device)\n",
        "            print(args.summary)\n",
        "            if acc > best_acc and not args.eval_only:\n",
        "              torch.save(model.state_dict(), './drive/My Drive/MPCooking 2/ckpt/' + args.model_name + '.pkl')\n",
        "              best_acc = acc\n",
        "        if args.eval_only:\n",
        "            print('Done Eval!')\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Loss: 12.341\n",
            "Iteration: 1, Loss: 12.466\n",
            "Iteration: 2, Loss: 47.814\n",
            "Iteration: 3, Loss: 12.526\n",
            "Iteration: 4, Loss: 47.003\n",
            "Iteration: 5, Loss: 12.420\n",
            "Iteration: 6, Loss: 46.068\n",
            "Iteration: 7, Loss: 12.493\n",
            "Iteration: 8, Loss: 45.321\n",
            "Iteration: 9, Loss: 12.459\n",
            "Iteration: 10, Loss: 44.563\n",
            "Iteration: 11, Loss: 12.341\n",
            "Iteration: 12, Loss: 43.535\n",
            "Iteration: 13, Loss: 12.367\n",
            "Iteration: 14, Loss: 42.755\n",
            "Iteration: 15, Loss: 12.373\n",
            "Iteration: 16, Loss: 41.712\n",
            "Iteration: 17, Loss: 12.396\n",
            "Iteration: 18, Loss: 40.798\n",
            "Iteration: 19, Loss: 12.318\n",
            "Iteration: 20, Loss: 40.169\n",
            "Iteration: 21, Loss: 12.299\n",
            "Iteration: 22, Loss: 39.035\n",
            "Iteration: 23, Loss: 12.307\n",
            "Iteration: 24, Loss: 38.219\n",
            "Iteration: 25, Loss: 12.416\n",
            "Iteration: 26, Loss: 37.411\n",
            "Iteration: 27, Loss: 12.343\n",
            "Iteration: 28, Loss: 36.471\n",
            "Iteration: 29, Loss: 12.293\n",
            "Iteration: 30, Loss: 35.763\n",
            "Iteration: 31, Loss: 12.316\n",
            "Iteration: 32, Loss: 34.902\n",
            "Iteration: 33, Loss: 12.249\n",
            "Iteration: 34, Loss: 34.186\n",
            "Iteration: 35, Loss: 12.295\n",
            "Iteration: 36, Loss: 33.416\n",
            "Iteration: 37, Loss: 12.198\n",
            "Iteration: 38, Loss: 32.724\n",
            "Iteration: 39, Loss: 12.222\n",
            "Iteration: 40, Loss: 32.024\n",
            "Iteration: 41, Loss: 12.228\n",
            "Iteration: 42, Loss: 31.198\n",
            "Iteration: 43, Loss: 12.249\n",
            "Iteration: 44, Loss: 30.531\n",
            "Iteration: 45, Loss: 12.348\n",
            "Iteration: 46, Loss: 29.954\n",
            "Iteration: 47, Loss: 12.232\n",
            "Iteration: 48, Loss: 29.212\n",
            "Iteration: 49, Loss: 12.271\n",
            "Iteration: 50, Loss: 28.739\n",
            "Iteration: 51, Loss: 12.275\n",
            "Iteration: 52, Loss: 28.109\n",
            "Iteration: 53, Loss: 12.253\n",
            "Iteration: 54, Loss: 27.657\n",
            "Iteration: 55, Loss: 12.236\n",
            "Iteration: 56, Loss: 26.929\n",
            "Iteration: 57, Loss: 12.183\n",
            "Iteration: 58, Loss: 26.453\n",
            "Iteration: 59, Loss: 12.195\n",
            "Iteration: 60, Loss: 25.981\n",
            "Iteration: 61, Loss: 12.213\n",
            "Iteration: 62, Loss: 25.461\n",
            "Iteration: 63, Loss: 12.186\n",
            "Iteration: 64, Loss: 25.094\n",
            "Iteration: 65, Loss: 12.170\n",
            "Iteration: 66, Loss: 24.612\n",
            "Iteration: 67, Loss: 12.045\n",
            "Iteration: 68, Loss: 24.250\n",
            "Iteration: 69, Loss: 12.203\n",
            "Iteration: 70, Loss: 23.826\n",
            "Iteration: 71, Loss: 12.057\n",
            "Iteration: 72, Loss: 23.500\n",
            "Iteration: 73, Loss: 12.121\n",
            "Iteration: 74, Loss: 23.186\n",
            "Iteration: 75, Loss: 12.179\n",
            "Iteration: 76, Loss: 22.650\n",
            "Iteration: 77, Loss: 12.165\n",
            "Iteration: 78, Loss: 22.321\n",
            "Iteration: 79, Loss: 12.108\n",
            "Iteration: 80, Loss: 22.080\n",
            "Iteration: 81, Loss: 12.095\n",
            "Iteration: 82, Loss: 21.782\n",
            "Iteration: 83, Loss: 12.078\n",
            "Iteration: 84, Loss: 21.438\n",
            "Iteration: 85, Loss: 12.078\n",
            "Iteration: 86, Loss: 21.120\n",
            "Iteration: 87, Loss: 12.115\n",
            "Iteration: 88, Loss: 20.808\n",
            "Iteration: 89, Loss: 12.092\n",
            "Iteration: 90, Loss: 20.627\n",
            "Iteration: 91, Loss: 12.130\n",
            "Iteration: 92, Loss: 20.270\n",
            "Iteration: 93, Loss: 12.084\n",
            "Iteration: 94, Loss: 20.079\n",
            "Iteration: 95, Loss: 12.022\n",
            "Iteration: 96, Loss: 19.866\n",
            "Iteration: 97, Loss: 12.093\n",
            "Iteration: 98, Loss: 19.755\n",
            "Iteration: 99, Loss: 12.200\n",
            "Iteration: 100, Loss: 19.442\n",
            "Iteration: 101, Loss: 12.032\n",
            "Iteration: 102, Loss: 19.319\n",
            "Iteration: 103, Loss: 12.021\n",
            "Iteration: 104, Loss: 19.218\n",
            "Iteration: 105, Loss: 12.121\n",
            "Iteration: 106, Loss: 19.000\n",
            "Iteration: 107, Loss: 12.137\n",
            "Iteration: 108, Loss: 18.708\n",
            "Iteration: 109, Loss: 12.048\n",
            "Iteration: 110, Loss: 18.626\n",
            "Iteration: 111, Loss: 12.051\n",
            "Iteration: 112, Loss: 18.362\n",
            "Iteration: 113, Loss: 12.045\n",
            "Iteration: 114, Loss: 18.342\n",
            "Iteration: 115, Loss: 12.011\n",
            "Iteration: 116, Loss: 18.275\n",
            "Iteration: 117, Loss: 12.135\n",
            "Iteration: 118, Loss: 18.134\n",
            "Iteration: 119, Loss: 12.010\n",
            "Iteration: 120, Loss: 18.009\n",
            "Iteration: 121, Loss: 12.023\n",
            "Iteration: 122, Loss: 17.995\n",
            "Iteration: 123, Loss: 12.014\n",
            "Iteration: 124, Loss: 17.756\n",
            "Iteration: 125, Loss: 12.003\n",
            "Iteration: 126, Loss: 17.845\n",
            "Iteration: 127, Loss: 12.018\n",
            "Iteration: 128, Loss: 17.601\n",
            "Iteration: 129, Loss: 12.065\n",
            "Iteration: 130, Loss: 17.569\n",
            "Iteration: 131, Loss: 12.013\n",
            "Iteration: 132, Loss: 17.466\n",
            "Iteration: 133, Loss: 12.018\n",
            "Iteration: 134, Loss: 17.421\n",
            "Iteration: 135, Loss: 12.056\n",
            "Iteration: 136, Loss: 17.309\n",
            "Iteration: 137, Loss: 12.029\n",
            "Iteration: 138, Loss: 17.196\n",
            "Iteration: 139, Loss: 11.984\n",
            "Iteration: 140, Loss: 17.147\n",
            "Iteration: 141, Loss: 12.049\n",
            "Iteration: 142, Loss: 17.045\n",
            "Iteration: 143, Loss: 12.060\n",
            "Iteration: 144, Loss: 17.122\n",
            "Iteration: 145, Loss: 12.040\n",
            "Iteration: 146, Loss: 17.062\n",
            "Iteration: 147, Loss: 12.074\n",
            "Iteration: 148, Loss: 16.950\n",
            "Iteration: 149, Loss: 12.024\n",
            "Iteration: 150, Loss: 16.836\n",
            "Iteration: 151, Loss: 11.978\n",
            "Iteration: 152, Loss: 16.822\n",
            "Iteration: 153, Loss: 12.044\n",
            "Iteration: 154, Loss: 16.828\n",
            "Iteration: 155, Loss: 11.997\n",
            "Iteration: 156, Loss: 16.772\n",
            "Iteration: 157, Loss: 11.971\n",
            "Iteration: 158, Loss: 16.700\n",
            "Iteration: 159, Loss: 11.960\n",
            "Iteration: 160, Loss: 16.593\n",
            "Iteration: 161, Loss: 12.053\n",
            "Iteration: 162, Loss: 16.581\n",
            "Iteration: 163, Loss: 12.009\n",
            "Iteration: 164, Loss: 16.525\n",
            "Iteration: 165, Loss: 11.932\n",
            "Iteration: 166, Loss: 16.526\n",
            "Iteration: 167, Loss: 12.005\n",
            "Iteration: 168, Loss: 16.487\n",
            "Iteration: 169, Loss: 11.951\n",
            "Iteration: 170, Loss: 16.463\n",
            "Iteration: 171, Loss: 11.980\n",
            "Iteration: 172, Loss: 16.406\n",
            "Iteration: 173, Loss: 11.984\n",
            "Iteration: 174, Loss: 16.322\n",
            "Iteration: 175, Loss: 12.010\n",
            "Iteration: 176, Loss: 16.398\n",
            "Iteration: 177, Loss: 12.019\n",
            "Iteration: 178, Loss: 16.228\n",
            "Iteration: 179, Loss: 11.931\n",
            "Iteration: 180, Loss: 16.276\n",
            "Iteration: 181, Loss: 11.983\n",
            "Iteration: 182, Loss: 16.227\n",
            "Iteration: 183, Loss: 12.028\n",
            "Iteration: 184, Loss: 16.257\n",
            "Iteration: 185, Loss: 12.013\n",
            "Iteration: 186, Loss: 16.156\n",
            "Iteration: 187, Loss: 11.920\n",
            "Iteration: 188, Loss: 16.148\n",
            "Iteration: 189, Loss: 11.998\n",
            "Iteration: 190, Loss: 16.128\n",
            "Iteration: 191, Loss: 11.986\n",
            "Iteration: 192, Loss: 16.126\n",
            "Iteration: 193, Loss: 11.969\n",
            "Iteration: 194, Loss: 16.084\n",
            "Iteration: 195, Loss: 11.969\n",
            "Iteration: 196, Loss: 15.986\n",
            "Iteration: 197, Loss: 12.006\n",
            "Iteration: 198, Loss: 15.966\n",
            "Iteration: 199, Loss: 11.911\n",
            "Iteration: 200, Loss: 15.989\n",
            "Iteration: 201, Loss: 11.892\n",
            "Iteration: 202, Loss: 15.847\n",
            "Iteration: 203, Loss: 11.945\n",
            "Iteration: 204, Loss: 15.843\n",
            "Iteration: 205, Loss: 11.946\n",
            "Iteration: 206, Loss: 15.892\n",
            "Iteration: 207, Loss: 11.912\n",
            "Iteration: 208, Loss: 15.779\n",
            "Iteration: 209, Loss: 11.846\n",
            "Iteration: 210, Loss: 15.718\n",
            "Iteration: 211, Loss: 11.873\n",
            "Iteration: 212, Loss: 15.728\n",
            "Iteration: 213, Loss: 11.872\n",
            "Iteration: 214, Loss: 15.747\n",
            "Iteration: 215, Loss: 11.950\n",
            "Iteration: 216, Loss: 15.662\n",
            "Iteration: 217, Loss: 11.997\n",
            "Iteration: 218, Loss: 15.685\n",
            "Iteration: 219, Loss: 11.956\n",
            "Iteration: 220, Loss: 15.667\n",
            "Iteration: 221, Loss: 11.946\n",
            "Iteration: 222, Loss: 15.623\n",
            "Iteration: 223, Loss: 11.887\n",
            "Iteration: 224, Loss: 15.644\n",
            "Iteration: 225, Loss: 11.826\n",
            "Iteration: 226, Loss: 15.628\n",
            "Iteration: 227, Loss: 11.937\n",
            "Iteration: 228, Loss: 15.611\n",
            "Iteration: 229, Loss: 11.853\n",
            "Iteration: 230, Loss: 15.635\n",
            "Iteration: 231, Loss: 11.861\n",
            "Iteration: 232, Loss: 15.518\n",
            "Iteration: 233, Loss: 11.894\n",
            "Iteration: 234, Loss: 15.565\n",
            "Iteration: 235, Loss: 11.870\n",
            "Iteration: 236, Loss: 15.478\n",
            "Iteration: 237, Loss: 11.963\n",
            "Iteration: 238, Loss: 15.462\n",
            "Iteration: 239, Loss: 11.893\n",
            "Iteration: 240, Loss: 15.477\n",
            "Iteration: 241, Loss: 11.822\n",
            "Iteration: 242, Loss: 15.386\n",
            "Iteration: 243, Loss: 11.924\n",
            "Iteration: 244, Loss: 15.413\n",
            "Iteration: 245, Loss: 11.930\n",
            "Iteration: 246, Loss: 15.456\n",
            "Iteration: 247, Loss: 11.910\n",
            "Iteration: 248, Loss: 15.404\n",
            "Iteration: 249, Loss: 11.791\n",
            "Iteration: 250, Loss: 15.393\n",
            "Iteration: 251, Loss: 11.865\n",
            "Iteration: 252, Loss: 15.258\n",
            "Iteration: 253, Loss: 11.813\n",
            "Iteration: 254, Loss: 15.293\n",
            "Iteration: 255, Loss: 11.826\n",
            "Iteration: 256, Loss: 15.441\n",
            "Iteration: 257, Loss: 11.920\n",
            "Iteration: 258, Loss: 15.289\n",
            "Iteration: 259, Loss: 11.921\n",
            "Iteration: 260, Loss: 15.205\n",
            "Iteration: 261, Loss: 11.835\n",
            "Iteration: 262, Loss: 15.266\n",
            "Iteration: 263, Loss: 11.840\n",
            "Iteration: 264, Loss: 15.215\n",
            "Iteration: 265, Loss: 11.803\n",
            "Iteration: 266, Loss: 15.116\n",
            "Iteration: 267, Loss: 11.908\n",
            "Iteration: 268, Loss: 15.239\n",
            "Iteration: 269, Loss: 11.862\n",
            "Iteration: 270, Loss: 15.108\n",
            "Iteration: 271, Loss: 11.914\n",
            "Iteration: 272, Loss: 15.214\n",
            "Iteration: 273, Loss: 11.883\n",
            "Iteration: 274, Loss: 15.090\n",
            "Iteration: 275, Loss: 11.867\n",
            "Iteration: 276, Loss: 15.078\n",
            "Iteration: 277, Loss: 11.840\n",
            "Iteration: 278, Loss: 15.104\n",
            "Iteration: 279, Loss: 11.868\n",
            "Iteration: 280, Loss: 15.072\n",
            "Iteration: 281, Loss: 11.881\n",
            "Iteration: 282, Loss: 15.018\n",
            "Iteration: 283, Loss: 11.828\n",
            "Iteration: 284, Loss: 15.063\n",
            "Iteration: 285, Loss: 11.889\n",
            "Iteration: 286, Loss: 14.985\n",
            "Iteration: 287, Loss: 11.802\n",
            "Iteration: 288, Loss: 14.985\n",
            "Iteration: 289, Loss: 11.838\n",
            "Iteration: 290, Loss: 14.968\n",
            "Iteration: 291, Loss: 11.783\n",
            "Iteration: 292, Loss: 14.951\n",
            "Iteration: 293, Loss: 11.745\n",
            "Iteration: 294, Loss: 14.974\n",
            "Iteration: 295, Loss: 11.713\n",
            "Iteration: 296, Loss: 14.922\n",
            "Iteration: 297, Loss: 11.769\n",
            "Iteration: 298, Loss: 14.916\n",
            "Iteration: 299, Loss: 11.893\n",
            "Iteration: 300, Loss: 14.892\n",
            "Iteration: 301, Loss: 11.693\n",
            "Iteration: 302, Loss: 14.858\n",
            "Iteration: 303, Loss: 11.738\n",
            "Iteration: 304, Loss: 14.858\n",
            "Iteration: 305, Loss: 11.809\n",
            "Iteration: 306, Loss: 14.843\n",
            "Iteration: 307, Loss: 11.826\n",
            "Iteration: 308, Loss: 14.764\n",
            "Iteration: 309, Loss: 11.779\n",
            "Iteration: 310, Loss: 14.898\n",
            "Iteration: 311, Loss: 11.859\n",
            "Iteration: 312, Loss: 14.851\n",
            "Iteration: 313, Loss: 11.745\n",
            "Iteration: 314, Loss: 14.833\n",
            "Iteration: 315, Loss: 11.749\n",
            "Iteration: 316, Loss: 14.772\n",
            "Iteration: 317, Loss: 11.785\n",
            "Iteration: 318, Loss: 14.745\n",
            "Iteration: 319, Loss: 11.793\n",
            "Iteration: 320, Loss: 14.746\n",
            "Iteration: 321, Loss: 11.794\n",
            "Iteration: 322, Loss: 14.726\n",
            "Iteration: 323, Loss: 11.754\n",
            "Iteration: 324, Loss: 14.743\n",
            "Iteration: 325, Loss: 11.793\n",
            "Iteration: 326, Loss: 14.737\n",
            "Iteration: 327, Loss: 11.775\n",
            "Iteration: 328, Loss: 14.678\n",
            "Iteration: 329, Loss: 11.737\n",
            "Iteration: 330, Loss: 14.658\n",
            "Iteration: 331, Loss: 11.826\n",
            "Iteration: 332, Loss: 14.648\n",
            "Iteration: 333, Loss: 11.729\n",
            "Iteration: 334, Loss: 14.676\n",
            "Iteration: 335, Loss: 11.728\n",
            "Iteration: 336, Loss: 14.567\n",
            "Iteration: 337, Loss: 11.797\n",
            "Iteration: 338, Loss: 14.663\n",
            "Iteration: 339, Loss: 11.847\n",
            "Iteration: 340, Loss: 14.652\n",
            "Iteration: 341, Loss: 11.817\n",
            "Iteration: 342, Loss: 14.614\n",
            "Iteration: 343, Loss: 11.735\n",
            "Iteration: 344, Loss: 14.646\n",
            "Iteration: 345, Loss: 11.707\n",
            "Iteration: 346, Loss: 14.578\n",
            "Iteration: 347, Loss: 11.749\n",
            "Iteration: 348, Loss: 14.581\n",
            "Iteration: 349, Loss: 11.733\n",
            "Iteration: 350, Loss: 14.552\n",
            "Iteration: 351, Loss: 11.720\n",
            "Iteration: 352, Loss: 14.640\n",
            "Iteration: 353, Loss: 11.699\n",
            "Iteration: 354, Loss: 14.503\n",
            "Iteration: 355, Loss: 11.674\n",
            "Iteration: 356, Loss: 14.549\n",
            "Iteration: 357, Loss: 11.836\n",
            "Iteration: 358, Loss: 14.499\n",
            "Iteration: 359, Loss: 11.662\n",
            "Iteration: 360, Loss: 14.466\n",
            "Iteration: 361, Loss: 11.683\n",
            "Iteration: 362, Loss: 14.489\n",
            "Iteration: 363, Loss: 11.728\n",
            "Iteration: 364, Loss: 14.489\n",
            "Iteration: 365, Loss: 11.703\n",
            "Iteration: 366, Loss: 14.429\n",
            "Iteration: 367, Loss: 11.695\n",
            "Iteration: 368, Loss: 14.439\n",
            "Iteration: 369, Loss: 11.691\n",
            "Iteration: 370, Loss: 14.481\n",
            "Iteration: 371, Loss: 11.667\n",
            "Iteration: 372, Loss: 14.408\n",
            "Iteration: 373, Loss: 11.657\n",
            "Iteration: 374, Loss: 14.447\n",
            "Iteration: 375, Loss: 11.603\n",
            "Iteration: 376, Loss: 14.459\n",
            "Iteration: 377, Loss: 11.612\n",
            "Iteration: 378, Loss: 14.344\n",
            "Iteration: 379, Loss: 11.615\n",
            "Iteration: 380, Loss: 14.445\n",
            "Iteration: 381, Loss: 11.711\n",
            "Iteration: 382, Loss: 14.370\n",
            "Iteration: 383, Loss: 11.784\n",
            "Iteration: 384, Loss: 14.399\n",
            "Iteration: 385, Loss: 11.645\n",
            "Iteration: 386, Loss: 14.386\n",
            "Iteration: 387, Loss: 11.696\n",
            "Iteration: 388, Loss: 14.348\n",
            "Iteration: 389, Loss: 11.706\n",
            "Iteration: 390, Loss: 14.364\n",
            "Iteration: 391, Loss: 11.696\n",
            "Iteration: 392, Loss: 14.440\n",
            "Iteration: 393, Loss: 11.617\n",
            "Iteration: 394, Loss: 14.263\n",
            "Iteration: 395, Loss: 11.716\n",
            "Iteration: 396, Loss: 14.308\n",
            "Iteration: 397, Loss: 11.729\n",
            "Iteration: 398, Loss: 14.328\n",
            "Iteration: 399, Loss: 11.730\n",
            "Iteration: 400, Loss: 14.324\n",
            "Iteration: 401, Loss: 11.589\n",
            "Iteration: 402, Loss: 14.229\n",
            "Iteration: 403, Loss: 11.667\n",
            "Iteration: 404, Loss: 14.272\n",
            "Iteration: 405, Loss: 11.628\n",
            "Iteration: 406, Loss: 14.303\n",
            "Iteration: 407, Loss: 11.596\n",
            "Iteration: 408, Loss: 14.343\n",
            "Iteration: 409, Loss: 11.700\n",
            "Iteration: 410, Loss: 14.248\n",
            "Iteration: 411, Loss: 11.646\n",
            "Iteration: 412, Loss: 14.236\n",
            "Iteration: 413, Loss: 11.684\n",
            "Iteration: 414, Loss: 14.258\n",
            "Iteration: 415, Loss: 11.534\n",
            "Iteration: 416, Loss: 14.261\n",
            "Iteration: 417, Loss: 11.535\n",
            "Iteration: 418, Loss: 14.142\n",
            "Iteration: 419, Loss: 11.548\n",
            "Iteration: 420, Loss: 14.229\n",
            "Iteration: 421, Loss: 11.538\n",
            "Iteration: 422, Loss: 14.215\n",
            "Iteration: 423, Loss: 11.705\n",
            "Iteration: 424, Loss: 14.196\n",
            "Iteration: 425, Loss: 11.613\n",
            "Iteration: 426, Loss: 14.189\n",
            "Iteration: 427, Loss: 11.536\n",
            "Iteration: 428, Loss: 14.075\n",
            "Iteration: 429, Loss: 11.588\n",
            "Iteration: 430, Loss: 14.084\n",
            "Iteration: 431, Loss: 11.611\n",
            "Iteration: 432, Loss: 14.195\n",
            "Iteration: 433, Loss: 11.633\n",
            "Iteration: 434, Loss: 14.130\n",
            "Iteration: 435, Loss: 11.566\n",
            "Iteration: 436, Loss: 14.126\n",
            "Iteration: 437, Loss: 11.699\n",
            "Iteration: 438, Loss: 14.172\n",
            "Iteration: 439, Loss: 11.571\n",
            "Iteration: 440, Loss: 14.113\n",
            "Iteration: 441, Loss: 11.585\n",
            "Iteration: 442, Loss: 14.087\n",
            "Iteration: 443, Loss: 11.495\n",
            "Iteration: 444, Loss: 14.090\n",
            "Iteration: 445, Loss: 11.494\n",
            "Iteration: 446, Loss: 14.112\n",
            "Iteration: 447, Loss: 11.431\n",
            "Iteration: 448, Loss: 14.105\n",
            "Iteration: 449, Loss: 11.502\n",
            "Iteration: 450, Loss: 14.062\n",
            "Iteration: 451, Loss: 11.617\n",
            "Iteration: 452, Loss: 14.048\n",
            "Iteration: 453, Loss: 11.504\n",
            "Iteration: 454, Loss: 14.022\n",
            "Iteration: 455, Loss: 11.485\n",
            "Iteration: 456, Loss: 14.096\n",
            "Iteration: 457, Loss: 11.729\n",
            "Iteration: 458, Loss: 14.077\n",
            "Iteration: 459, Loss: 11.534\n",
            "Iteration: 460, Loss: 14.080\n",
            "Iteration: 461, Loss: 11.658\n",
            "Iteration: 462, Loss: 13.964\n",
            "Iteration: 463, Loss: 11.556\n",
            "Iteration: 464, Loss: 13.977\n",
            "Iteration: 465, Loss: 11.431\n",
            "Iteration: 466, Loss: 14.024\n",
            "Iteration: 467, Loss: 11.578\n",
            "Iteration: 468, Loss: 14.025\n",
            "Iteration: 469, Loss: 11.616\n",
            "Iteration: 470, Loss: 14.048\n",
            "Iteration: 471, Loss: 11.552\n",
            "Iteration: 472, Loss: 13.939\n",
            "Iteration: 473, Loss: 11.522\n",
            "Iteration: 474, Loss: 13.989\n",
            "Iteration: 475, Loss: 11.538\n",
            "Iteration: 476, Loss: 14.003\n",
            "Iteration: 477, Loss: 11.494\n",
            "Iteration: 478, Loss: 14.036\n",
            "Iteration: 479, Loss: 11.497\n",
            "Iteration: 480, Loss: 13.989\n",
            "Iteration: 481, Loss: 11.407\n",
            "Iteration: 482, Loss: 13.929\n",
            "Iteration: 483, Loss: 11.389\n",
            "Iteration: 484, Loss: 13.935\n",
            "Iteration: 485, Loss: 11.534\n",
            "Iteration: 486, Loss: 13.972\n",
            "Iteration: 487, Loss: 11.517\n",
            "Iteration: 488, Loss: 13.879\n",
            "Iteration: 489, Loss: 11.527\n",
            "Iteration: 490, Loss: 13.918\n",
            "Iteration: 491, Loss: 11.615\n",
            "Iteration: 492, Loss: 13.856\n",
            "Iteration: 493, Loss: 11.562\n",
            "Iteration: 494, Loss: 13.893\n",
            "Iteration: 495, Loss: 11.642\n",
            "Iteration: 496, Loss: 13.896\n",
            "Iteration: 497, Loss: 11.499\n",
            "Iteration: 498, Loss: 13.898\n",
            "Iteration: 499, Loss: 11.454\n",
            "Iteration: 500, Loss: 13.865\n",
            "Testing test data point 0 of 42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "classificationMAP.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  rec=tp/npos; prec=tp/(fp+tp)\n",
            "classificationMAP.py:13: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.sum(tmp*prec)/npos\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dacfdb193c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/test.pyc\u001b[0m in \u001b[0;36mtest\u001b[0;34m(itr, dataset, args, model, logger, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_logits_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mdmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdmAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcam_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlst_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classification map %f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: ConvertDataset instance has no attribute 'path_to_annotations'"
          ]
        }
      ]
    }
  ]
}